{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b90a52e5",
   "metadata": {},
   "source": [
    "# AE4350 Assignment : Car Racing Gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6200b64a",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "631c2fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "import shimmy\n",
    "import gymnasium\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import copy\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c54c17",
   "metadata": {},
   "source": [
    "## 2. Create Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03402b88",
   "metadata": {},
   "source": [
    "This section defines a custom environment wrapper class, `BasicWrapper`, that extends the functionality of the Gym environment. The wrapper includes features such as preprocessing observations, modifying rewards based on specific conditions, and managing timers and flags for various in-game events. The wrapper also handles resetting the environment and rendering it. Additionally, a helper function, `rgb2gray`, is provided to convert RGB images to grayscale using the luminosity method.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5c00a7",
   "metadata": {},
   "source": [
    "### Creating Environment Wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f80767",
   "metadata": {},
   "source": [
    "Helper function to convert RGB to grayscale using luminosity method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b53a565b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb2gray(rgb):\n",
    "    return np.dot(rgb[..., :3], [0.2989, 0.5870, 0.1140])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49675eb",
   "metadata": {},
   "source": [
    "Custom environment wrapper class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0ed6a726",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicWrapper(gymnasium.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.grass_timer = 0\n",
    "        self.grass_detected = False\n",
    "        self.gas_timer = 0\n",
    "        self.no_gas = False\n",
    "        self.prev_observation = None  # Reset the previous observation\n",
    "        self.prevprev_observation = None  # Reset the previous observation\n",
    "        self.observation_space = gymnasium.spaces.Box(low=0, high=255, shape=(84, 96,3), dtype=np.uint8)\n",
    "        self.action_space = gymnasium.spaces.Discrete(13)  # Updated action space\n",
    "        self.score  = 0\n",
    "\n",
    "    def _preprocess_observation(self, observation):\n",
    "        \"\"\"\n",
    "        Preprocess the observation by removing bottom bar, converting to grayscale, and combining frames.\n",
    "\n",
    "        Parameters:\n",
    "        observation (numpy.ndarray): The current RGB observation from the environment with shape (height, width, 3).\n",
    "\n",
    "        Returns:\n",
    "        numpy.ndarray: Preprocessed observation with stacked frames and grayscale, with shape (height, width, 3*num_frames).\n",
    "        \"\"\"\n",
    "        # Remove the bottom bar from the observation\n",
    "        mod_obs = observation[:84, :, :]\n",
    "\n",
    "        # Convert observation to grayscale\n",
    "        mod_obs = rgb2gray(mod_obs)\n",
    "        mod_obs = np.expand_dims(mod_obs, axis=-1)\n",
    "\n",
    "        if self.prevprev_observation is not None:\n",
    "            # Combine the current observation and previous observation\n",
    "            combined_obs = np.concatenate([self.prevprev_observation.copy(),self.prev_observation.copy(), mod_obs.copy()], axis=-1)\n",
    "        elif  self.prev_observation is not None:\n",
    "            combined_obs = np.concatenate([self.prev_observation.copy(), mod_obs.copy(), mod_obs.copy()], axis=-1)\n",
    "        else:\n",
    "            # If there is no previous observation, use the current observation as is\n",
    "            combined_obs = np.concatenate([mod_obs.copy(), mod_obs.copy(), mod_obs.copy()], axis=-1)\n",
    "\n",
    "\n",
    "        # Update previous observation values\n",
    "        self.prevprev_observation = copy.deepcopy(self.prev_observation)\n",
    "        self.prev_observation = copy.deepcopy(mod_obs)\n",
    "\n",
    "        return combined_obs\n",
    "    \n",
    "    def modify_reward(self,reward, action, obs):\n",
    "        \"\"\"\n",
    "        Modify the reward based on various conditions.\n",
    "\n",
    "        Parameters:\n",
    "        reward (float): The original reward value.\n",
    "        action (numpy.ndarray): The action taken in the environment.\n",
    "        obs (numpy.ndarray): The current observation from the environment with shape (height, width, 3).\n",
    "        self (BasicWrapper): The instance of the BasicWrapper class.\n",
    "\n",
    "        Returns:\n",
    "        float: The modified reward value based on different conditions.\n",
    "        \"\"\"\n",
    "        # Clipping the reward\n",
    "        reward = np.clip(reward, a_max=1.0, a_min=None)\n",
    "\n",
    "        # No gas timer\n",
    "        if action[1] - action[2] <= 0.3:\n",
    "            self.no_gas = True\n",
    "        else:\n",
    "            self.no_gas = False\n",
    "\n",
    "        if self.no_gas:\n",
    "            self.gas_timer += 1\n",
    "        else:\n",
    "            self.gas_timer = 0\n",
    "\n",
    "        if self.gas_timer >= 5:\n",
    "            reward -= 0.5\n",
    "\n",
    "        # Grass time out\n",
    "        if np.any(obs[67:77, 46:50, 1] / 255 > 0.5):\n",
    "            self.grass_detected = True\n",
    "        else:\n",
    "            self.grass_detected = False\n",
    "\n",
    "        if self.grass_detected:\n",
    "            self.grass_timer += 1\n",
    "        else:\n",
    "            self.grass_timer = 0\n",
    "\n",
    "        if self.grass_timer > 4:\n",
    "            reward -= 0.1\n",
    "        if self.grass_timer > 20:\n",
    "            reward -= 0.5\n",
    "\n",
    "        return reward\n",
    "    \n",
    "    def action_list(self,action):\n",
    "        \n",
    "        if action == 0:\n",
    "            cont = [0,0,0]\n",
    "        elif  action  == 1:\n",
    "            cont = [-1, 0, 0]\n",
    "        elif action == 2:\n",
    "            cont = [1, 0, 0]\n",
    "        elif action  == 3:\n",
    "            cont = [0, 1, 0]\n",
    "        elif action == 4:\n",
    "            cont = [0,0,0.8]\n",
    "        elif action == 5:\n",
    "            cont = [-0.5, 0.5, 0]\n",
    "        elif action == 6:\n",
    "            cont = [0.5, 0.5, 0]\n",
    "        elif action == 7:\n",
    "            cont = [-0.5, 0, 0.5]\n",
    "        elif action == 8:\n",
    "            cont = [0.5, 0, 0.5]\n",
    "        elif action == 9:\n",
    "            cont = [0,1,0.5]\n",
    "        elif action == 10:\n",
    "            cont = [0,0.5,0.8]\n",
    "        elif action == 11:\n",
    "            cont = [-0.3, 0.2, 0]\n",
    "        elif action == 12:\n",
    "            cont = [0.3, 0.2, 0]\n",
    "        return cont\n",
    "\n",
    "    \n",
    "    def step(self, action):\n",
    "        \n",
    "        action_cont  = self.action_list(action)\n",
    "        \n",
    "        # Perform the action in the environment and retrieve the resulting state\n",
    "        obs, reward, done, trun, info = self.env.step(action_cont)\n",
    "\n",
    "        # Modify the reward based on different conditions\n",
    "        reward = self.modify_reward(reward, action_cont, obs)\n",
    "\n",
    "        # Preprocess the observation and return updated values\n",
    "        pre_obs = self._preprocess_observation(obs)\n",
    "        \n",
    "        return pre_obs, reward, done, trun, info\n",
    "\n",
    "\n",
    "    def render(self):\n",
    "        # Render the environment\n",
    "        return self.env.render()\n",
    "\n",
    "    def close(self):\n",
    "        # Close the environment\n",
    "        return self.env.close()\n",
    "\n",
    "    def reset(self, track_id=None, **kwargs):\n",
    "        observation, info = self.env.reset(**kwargs)\n",
    "        observation = self._preprocess_observation(observation)\n",
    "        self.prev_observation = None  # Reset the previous observation\n",
    "        self.prevprev_observation = None  # Reset the previous observation\n",
    "        if info is None:\n",
    "            info = {}\n",
    "            \n",
    "        self.grass_timer = 0\n",
    "        self.grass_detected = False\n",
    "        self.gas_timer = 0\n",
    "        self.no_gas = False\n",
    "        return observation, info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07cd536",
   "metadata": {},
   "source": [
    "## 3. Creating Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977ae2b9",
   "metadata": {},
   "source": [
    "This section defines the process of creating and training a model using the specified environment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a815bdd",
   "metadata": {},
   "source": [
    "### Creating Parallel environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8b8e57ac",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    }
   ],
   "source": [
    "# Creating Environment\n",
    "environment_name = 'CarRacing-v2'\n",
    "num_envs = 6  # Number of parallel environments\n",
    "\n",
    "# Create log path\n",
    "log_path = os.path.join('Training', 'Logs')\n",
    "\n",
    "# Create a function to create the environment to be used by DummyVecEnv\n",
    "def make_env():\n",
    "    make_kwargs = {'lap_complete_percent': 0.95, 'render_mode': 'human'}\n",
    "    env = shimmy.openai_gym_compatibility.GymV26CompatibilityV0(env_id=environment_name, make_kwargs=make_kwargs)\n",
    "    env = BasicWrapper(env)  # Wrap the gym environment with the BasicWrapper\n",
    "    return env\n",
    "\n",
    "# Create a list of environments\n",
    "env_list = [make_env for _ in range(num_envs)]\n",
    "\n",
    "# Create a vectorized environment using DummyVecEnv\n",
    "env = DummyVecEnv(env_list)\n",
    "\n",
    "\n",
    "# Create nn model\n",
    "model = PPO('CnnPolicy', env, verbose=1, tensorboard_log=log_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f953b16",
   "metadata": {},
   "source": [
    "Modifying learing rate of optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7c67d25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set maximum gradient norm\n",
    "max_grad_norm = 0.5\n",
    "# Retrieve PyTorch optimizer from the model\n",
    "optimizer = model.policy.optimizer\n",
    "\n",
    "\n",
    "# Modify optimizer to enable gradient clipping\n",
    "optimizer.clip_grad_norm = max_grad_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73edfb4",
   "metadata": {},
   "source": [
    "### Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99b97f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training\\Logs\\PPO_193\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 29    |\n",
      "|    iterations      | 1     |\n",
      "|    time_elapsed    | 420   |\n",
      "|    total_timesteps | 12288 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 27          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 892         |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017545039 |\n",
      "|    clip_fraction        | 0.237       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.26       |\n",
      "|    explained_variance   | -0.0143     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.32        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0157     |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 1.36        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 27          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 1355        |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012441036 |\n",
      "|    clip_fraction        | 0.191       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.27       |\n",
      "|    explained_variance   | 0.352       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.375       |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00424    |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 0.827       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 26          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 1822        |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012998939 |\n",
      "|    clip_fraction        | 0.164       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.28       |\n",
      "|    explained_variance   | 0.739       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.238       |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | 6.46e-05    |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 0.592       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 26          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 2288        |\n",
      "|    total_timesteps      | 61440       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013201122 |\n",
      "|    clip_fraction        | 0.159       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.26       |\n",
      "|    explained_variance   | 0.513       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.149       |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00171    |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 0.659       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 26          |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 2801        |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014217564 |\n",
      "|    clip_fraction        | 0.135       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.27       |\n",
      "|    explained_variance   | 0.814       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.204       |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.00428    |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 0.829       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 25          |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 3346        |\n",
      "|    total_timesteps      | 86016       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017456308 |\n",
      "|    clip_fraction        | 0.161       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.28       |\n",
      "|    explained_variance   | 0.924       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.1         |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0036     |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 0.582       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 25          |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 3888        |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014660935 |\n",
      "|    clip_fraction        | 0.175       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.29       |\n",
      "|    explained_variance   | 0.947       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.124       |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00251    |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 0.571       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 24          |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 4432        |\n",
      "|    total_timesteps      | 110592      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018240275 |\n",
      "|    clip_fraction        | 0.182       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.3        |\n",
      "|    explained_variance   | 0.969       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0942      |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0025     |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 0.56        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 24         |\n",
      "|    iterations           | 10         |\n",
      "|    time_elapsed         | 5020       |\n",
      "|    total_timesteps      | 122880     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01750516 |\n",
      "|    clip_fraction        | 0.189      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -4.29      |\n",
      "|    explained_variance   | 0.983      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0637     |\n",
      "|    n_updates            | 90         |\n",
      "|    policy_gradient_loss | -0.00243   |\n",
      "|    std                  | 1.01       |\n",
      "|    value_loss           | 0.362      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 23          |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 5647        |\n",
      "|    total_timesteps      | 135168      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017619247 |\n",
      "|    clip_fraction        | 0.189       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -4.29       |\n",
      "|    explained_variance   | 0.982       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.097       |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.00207    |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 0.396       |\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model.learn(total_timesteps=1500000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6421319f",
   "metadata": {},
   "source": [
    "Saving the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd4734ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m Org_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest Models\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPPO_fin3\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39msave(Org_path)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "Org_path = os.path.join('Training', 'Test Models', 'PPO_fin3')\n",
    "model.save(Org_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f871474",
   "metadata": {},
   "source": [
    "# 4. Evaluate Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b94bb0",
   "metadata": {},
   "source": [
    "Evaluation wrapper that pre-processes the observation such that the agent can work. Same as basicwrapper but without reward function altering such that the agent can evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3feca0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvalWrapper(gymnasium.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.grass_timer = 0\n",
    "        self.grass_detected = False\n",
    "        self.gas_timer = 0\n",
    "        self.no_gas = False\n",
    "        self.prev_observation = None  # Reset the previous observation\n",
    "        self.prevprev_observation = None  # Reset the previous observation\n",
    "        self.observation_space = gymnasium.spaces.Box(low=0, high=255, shape=(84, 96,3), dtype=np.uint8)\n",
    "        self.score  = 0\n",
    "\n",
    "    def _preprocess_observation(self, observation):\n",
    "        \"\"\"\n",
    "        Preprocess the observation by removing bottom bar, converting to grayscale, and combining frames.\n",
    "\n",
    "        Parameters:\n",
    "        observation (numpy.ndarray): The current RGB observation from the environment with shape (height, width, 3).\n",
    "\n",
    "        Returns:\n",
    "        numpy.ndarray: Preprocessed observation with stacked frames and grayscale, with shape (height, width, 3*num_frames).\n",
    "        \"\"\"\n",
    "        # Remove the bottom bar from the observation\n",
    "        mod_obs = observation[:84, :, :]\n",
    "\n",
    "        # Convert observation to grayscale\n",
    "        mod_obs = rgb2gray(mod_obs)\n",
    "        mod_obs = np.expand_dims(mod_obs, axis=-1)\n",
    "\n",
    "        if self.prevprev_observation is not None:\n",
    "            # Combine the current observation and previous observation\n",
    "            combined_obs = np.concatenate([self.prevprev_observation.copy(),self.prev_observation.copy(), mod_obs.copy()], axis=-1)\n",
    "        elif  self.prev_observation is not None:\n",
    "            combined_obs = np.concatenate([self.prev_observation.copy(), mod_obs.copy(), mod_obs.copy()], axis=-1)\n",
    "        else:\n",
    "            # If there is no previous observation, use the current observation as is\n",
    "            combined_obs = np.concatenate([mod_obs.copy(), mod_obs.copy(), mod_obs.copy()], axis=-1)\n",
    "\n",
    "\n",
    "        # Update previous observation values\n",
    "        self.prevprev_observation = copy.deepcopy(self.prev_observation)\n",
    "        self.prev_observation = copy.deepcopy(mod_obs)\n",
    "\n",
    "        return combined_obs\n",
    "    \n",
    "\n",
    "    def step(self, action):\n",
    "        # Perform the action in the environment and retrieve the resulting state\n",
    "        obs, reward, done, trun, info = self.env.step(action)\n",
    "\n",
    "        # Preprocess the observation and return updated values\n",
    "        pre_obs = self._preprocess_observation(obs)\n",
    "        \n",
    "        return pre_obs, reward, done, trun, info\n",
    "\n",
    "\n",
    "    def render(self):\n",
    "        # Render the environment\n",
    "        return self.env.render()\n",
    "\n",
    "    def close(self):\n",
    "        # Close the environment\n",
    "        return self.env.close()\n",
    "\n",
    "    def reset(self, track_id=None, **kwargs):\n",
    "        observation, info = self.env.reset(**kwargs)\n",
    "        observation = self._preprocess_observation(observation)\n",
    "        self.prev_observation = None  # Reset the previous observation\n",
    "        self.prevprev_observation = None  # Reset the previous observation\n",
    "        if info is None:\n",
    "            info = {}\n",
    "            \n",
    "        self.grass_timer = 0\n",
    "        self.grass_detected = False\n",
    "        self.gas_timer = 0\n",
    "        self.no_gas = False\n",
    "        return observation, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "023c2087",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 1.11 GiB for an array with shape (2048, 6, 3, 84, 96) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m Org_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest Models\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPPO_fin1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Load the trained model\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mPPO\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mOrg_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m environment_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCarRacing-v2\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Create the environment\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\stable_baselines3\\common\\base_class.py:741\u001b[0m, in \u001b[0;36mBaseAlgorithm.load\u001b[1;34m(cls, path, env, device, custom_objects, print_system_info, force_reset, **kwargs)\u001b[0m\n\u001b[0;32m    739\u001b[0m model\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\u001b[38;5;241m.\u001b[39mupdate(data)\n\u001b[0;32m    740\u001b[0m model\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\u001b[38;5;241m.\u001b[39mupdate(kwargs)\n\u001b[1;32m--> 741\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    744\u001b[0m     \u001b[38;5;66;03m# put state_dicts back in place\u001b[39;00m\n\u001b[0;32m    745\u001b[0m     model\u001b[38;5;241m.\u001b[39mset_parameters(params, exact_match\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:167\u001b[0m, in \u001b[0;36mPPO._setup_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_setup_model\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 167\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;66;03m# Initialize schedules for policy/value clipping\u001b[39;00m\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclip_range \u001b[38;5;241m=\u001b[39m get_schedule_fn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclip_range)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:113\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm._setup_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_random_seed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseed)\n\u001b[0;32m    111\u001b[0m buffer_cls \u001b[38;5;241m=\u001b[39m DictRolloutBuffer \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_space, spaces\u001b[38;5;241m.\u001b[39mDict) \u001b[38;5;28;01melse\u001b[39;00m RolloutBuffer\n\u001b[1;32m--> 113\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrollout_buffer \u001b[38;5;241m=\u001b[39m \u001b[43mbuffer_cls\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    115\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservation_space\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_space\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgae_lambda\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgae_lambda\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_envs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_envs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m# pytype:disable=not-instantiable\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy_class(  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_space, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_schedule, use_sde\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_sde, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy_kwargs\n\u001b[0;32m    125\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\stable_baselines3\\common\\buffers.py:361\u001b[0m, in \u001b[0;36mRolloutBuffer.__init__\u001b[1;34m(self, buffer_size, observation_space, action_space, device, gae_lambda, gamma, n_envs)\u001b[0m\n\u001b[0;32m    359\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;241m=\u001b[39m gamma\n\u001b[0;32m    360\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerator_ready \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 361\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\stable_baselines3\\common\\buffers.py:364\u001b[0m, in \u001b[0;36mRolloutBuffer.reset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    363\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreset\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 364\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservations \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuffer_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_envs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobs_shape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    365\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_envs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_dim), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m    366\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrewards \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_envs), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 1.11 GiB for an array with shape (2048, 6, 3, 84, 96) and data type float32"
     ]
    }
   ],
   "source": [
    "# Specify the environment name\n",
    "environment_name = 'CarRacing-v2'\n",
    "\n",
    "# Create the environment with specific settings\n",
    "make_kwargs = {'lap_complete_percent': 0.95, 'render_mode': 'human'}\n",
    "eval_env = shimmy.openai_gym_compatibility.GymV26CompatibilityV0(env_id=environment_name, make_kwargs=make_kwargs)\n",
    "\n",
    "# Wrap the environment with an evaluation wrapper\n",
    "eval_env = EvalWrapper(eval_env)\n",
    "\n",
    "# Function to test the trained model\n",
    "def test(model, env, n_eval_episodes=10, render=True):\n",
    "    \"\"\"\n",
    "    Evaluate the performance of a trained model on a given environment.\n",
    "\n",
    "    Parameters:\n",
    "        model (BaseAlgorithm): The trained reinforcement learning model to be tested.\n",
    "        env (gym.Env): The evaluation environment.\n",
    "        n_eval_episodes (int): Number of episodes to run for evaluation.\n",
    "        render (bool): Whether to render the environment during evaluation.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the average and standard deviation of episode rewards.\n",
    "    \"\"\"\n",
    "    episode_rewards = []\n",
    "\n",
    "    # Evaluate the model over a number of episodes\n",
    "    for _ in range(n_eval_episodes):\n",
    "        obs, info = env.reset()\n",
    "        done = False\n",
    "        trun = False\n",
    "        episode_reward = 0.0\n",
    "\n",
    "        # Run the simulation for one episode\n",
    "        while not done and not trun:\n",
    "            # Get the model's action prediction\n",
    "            action, _ = model.predict(obs.copy())\n",
    "            # Take a step in the environment\n",
    "            obs, reward, done, trun, _ = env.step(action)\n",
    "            episode_reward += reward\n",
    "            # Render the environment if specified\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "        episode_rewards.append(episode_reward)\n",
    "\n",
    "    # Calculate and return the average and standard deviation of episode rewards\n",
    "    return np.average(episode_rewards), np.std(episode_rewards)\n",
    "\n",
    "# Test the trained model using the evaluation environment\n",
    "a = test(model, eval_env, n_eval_episodes=5, render=True)\n",
    "\n",
    "# Close the evaluation environment\n",
    "eval_env.close()\n",
    "\n",
    "# Print the average and standard deviation of episode rewards\n",
    "print(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1709b0a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(773.164075564791, 112.73922707228856)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
